# random seed for batch sampling
seed: 42

exp_name: ???

valid_size: 128
# the batch size for training; for FSDP, the batch size per GPU is batch_size / (grad_accumulation_steps * num_gpus)
batch_size: 4
# the batch size during evaluation and sampling, if enabled
eval_batch_size: 16

# debug mode (disables wandb, model checkpointing, etc.)
debug: false

# the port to use for FSDP
fsdp_port: null

# to create the local run directory and cache models/datasets,
#   we will try each of these directories in order; if none exist,
#   we will create the last one and use it
local_dirs:
  - /scratch/mihalcea_root/mihalcea0/ajyl/ppo/models

# whether or not to generate samples during evaluation; disable for FSDP/TensorParallel
#   is recommended, because they are slow

# how many model samples to generate during evaluation
n_eval_model_samples: 16
n_epochs: 1

# an OmegaConf resolver that returns the local run directory, calling a function in utils.py
local_run_dir: ${get_local_run_dir:${exp_name},${local_dirs}}

# the maximum allowed length for an input (prompt + response)
max_length: 128
max_new_tokens: 64

# the maximum allowed length for a prompt
max_prompt_length: 64

# the trainer class to use (e.g. BasicTrainer, FSDPTrainer, TensorParallelTrainer)
evaluator: BasicEvaluator

sample_during_eval: false
sample_during_eval_wiki103: false

include_wiki103: true

defaults:
- _self_
- model: gpt2-medium
- data: gpt2-medium
- loss: dpo # which loss function, either sft or dpo (specify loss.beta if using dpo)
